---
title: "Homework 6"
author: "Anusorn Thanataveerat"
date: "November 15, 2018"
output: github_document
toc: true
toc_float: true
code_folding: hide
---

```{r setup, include=FALSE}
library(tidyverse)
library(janitor)
library(knitr)
library(broom)
library(modelr)
library(lubridate)

knitr::opts_chunk$set(echo = TRUE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1

``` {r Problem_1, message = FALSE}
homicide_dat <-
  read_csv('https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv') %>% 
  mutate(reported_date = ymd(reported_date), 
         city_state = paste(city, state, sep = ', '),
         victim_race = factor(ifelse(victim_race != 'White', 'non-white', 'white'),
        #Set white as reference group   
        levels = c('white', 'non-white')),
        victim_age = as.numeric(victim_age),
        resolved = ifelse(disposition == "Closed by arrest", 1, 0)) %>% 
        #fllter the following cities out
      filter(!(city_state %in% c('Dallas, TX', 'Phoenix, AZ', 'Kansas City, MO', 'Tulsa, AL')))
```

Look at the city of Baltimore data and fit the logistic regression

```{r baltimore}
baltimore_logistic <- homicide_dat %>% 
  filter(city_state == 'Baltimore, MD') %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race,
      family = binomial(link = 'logit'), data = .) 

 baltimore_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  bind_cols(.,  exp(confint_tidy(baltimore_logistic))) %>% 
  select(term, OR, conf.low, conf.high) %>% 
  filter(term == 'victim_racenon-white') %>% 
  knitr::kable(digits = 3)
```

The odds ratios of solving homicides comparing non-white victims to white victims keeping all other variables fixed is 0.44 (0.31, 0.62).


```{r warning = FALSE}
#Function of fitting glm and produce OR with CI
glm_or_ci_function <- function(dat){
  logistic <- glm(resolved ~ victim_age + victim_sex + victim_race,
      family = binomial(link = 'logit'), data = dat) 

 logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  bind_cols(.,  exp(confint_tidy(logistic))) %>% 
  select(term, OR, conf.low, conf.high) %>% 
  filter(term == 'victim_racenon-white')
}

 ORs_df <- homicide_dat %>% 
   select(victim_age, victim_race, victim_sex, resolved, city_state) %>% 
  nest(., victim_age:resolved) %>% 
   mutate(models = map(data, glm_or_ci_function)) 
 
 ORs_df %>% 
   select(-data) %>% 
   unnest() %>% 
   ggplot(aes(x = city_state, y = OR)) +
   geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +
   geom_hline(yintercept = 1, linetype = "dotdash", 
              color = "red", size = 0.6) +
   ylab('OR of resolved crime') + 
   xlab('') +
   ggtitle('non-white vs white') +
   theme(axis.text.x = element_text(color = "blue", size = 8, angle = 90))

```

Comment: main estimates: mostly below 1. those with statistically sig are xxx . some noteworthy city where the trend is reversed abc

## Problem 2
```{r data_cleaning}
dat <- read_csv('data/birthweight.csv') %>%
  mutate_at(c('babysex', 'frace', 'malform', 'mrace'), 
            funs(factor(.))) %>%
  mutate(smoke = as.factor(ifelse(smoken > 0, 1, 0)),
  id = row_number()) %>%
  select(-smoken)
  
#check for missing data
# skimr::skim(dat)
```

First create smoking binary variable from `smoken` since the data is zero heavy and wouldn't make much sense to try to establish a linear relationship between that and the outcome. Also, the data doesn't appear to contain missing information so we're good

Propose a regression model for birthweight: Lasso: takes care of predictors selection by regularizing the coefficient estimates of the predictors that are not predictive and also reduce the variance of the prediction. Data driven

```{r lasso}
library(glmnet)
#Prepare train, test data 
set.seed(1)
train_df = sample_frac(dat, 0.8)
test_df = anti_join(dat, train_df, by = "id")
#prepare data for the model - matrix of predictors
x_train <- model.matrix(bwt ~ . -id, train_df)[, -1] #don't want intercept column here
y_train <- train_df$bwt  #outcome of interest

#Find the best tuning parameter using cross-validation (minimize the MSE)
set.seed(2)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)

#The best lambda
best_cv <- glance(lasso_cv) %>% pull(lambda.min)

#fit the best lambda on the training set
lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = best_cv)
```

Walk through the steps of fitting the model

```{r}
lasso_mod %>% tidy() %>% kable()
```

explain a bit about the model. what remains and what's missing and what's the effect of each variable

plot of model residuals against fitted values on the test data from 3 models

```{r}
#Since add_predictions and residuals do not support the glmnet package, we'll take a different route
new_train_df <- train_df %>% 
  bind_cols(., predicted = predict(lasso_mod, x_train)) %>% 
  mutate(residuals = bwt - predicted)
# Residual = Observed value - Predicted value 
plot1 = ggplot(data = new_train_df, aes(x = predicted, y = residuals)) +
    geom_point() + ggtitle('Lasso')

#Consider other 2 models
#One using length at birth and gestational age as predictors (main effects only)
lin_main_mod <- lm(bwt ~ blength + gaweeks, data = train_df)
# One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
lin_interact_mod <- lm(bwt ~ blength*bhead*babysex, data = train_df)

plot2 = train_df %>% add_predictions(lin_main_mod) %>% 
  add_residuals(lin_main_mod) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + ggtitle('Lin_main_mod')

plot3 <- train_df %>% add_predictions(lin_interact_mod) %>% 
  add_residuals(lin_interact_mod) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + ggtitle('lin_interact_mod')



```

Next we comparison of 3 models using RMSE

```{r}
cv_df <- crossv_mc(dat, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

#Write function so that glmnet can operate in map context
lasso_mod_func <- function(train, test){
  #set up matrix for training set
  x <- model.matrix(bwt ~ . -id, train)[, -1] #don't want intercept column here
  y <- train$bwt  #outcome of interest
  lasso_mod <- glmnet(x, y, alpha = 1, lambda = best_cv)
  #set up matrix for test set
  x_test <- model.matrix(bwt ~ . -id, test)[, -1]
  y_test <- test$bwt
  predicted <- predict(lasso_mod, x_test)
  rmse_lasso_mod <- sqrt((sum(predicted - y_test)^2)/length(y_test))
  return(rmse_lasso_mod)
}

cv_df = 
  cv_df %>% 
  mutate(lin_main_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         lin_interact_mod = map(train, ~lm(bwt ~ blength*bhead*babysex, data = .x))) %>% 
  mutate(rmse_lin_main_mod = map2_dbl(lin_main_mod, test, ~rmse(model = .x, data = .y)),
         rmse_lin_interact_mod = map2_dbl(lin_interact_mod, test, ~rmse(model = .x, data = .y)),
         rmse_lasso_mod = map2_dbl(train, test, ~lasso_mod_func(train = .x, test = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Comment of the violin figure
